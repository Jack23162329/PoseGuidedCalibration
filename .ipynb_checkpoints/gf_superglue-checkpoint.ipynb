{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd505ca-5837-418b-961f-02d80f225189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on device \"cuda\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"indoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jack2\\Documents\\Final Project\\SuperGluePretrainedNetwork\\models\\superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n",
      "C:\\Users\\jack2\\Documents\\Final Project\\SuperGluePretrainedNetwork\\models\\superglue.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Processing image directory input: assets/freiburg_sequence/\n",
      "==> Will write outputs to dump_demo_sequence\n",
      "[Timer] data=0.005 forward=0.192 viz=0.008 total=0.205 sec {4.9 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000001.png\n",
      "[Timer] data=0.008 forward=0.159 viz=0.010 total=0.178 sec {5.6 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000002.png\n",
      "[Timer] data=0.011 forward=0.136 viz=0.011 total=0.159 sec {6.3 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000003.png\n",
      "[Timer] data=0.013 forward=0.121 viz=0.012 total=0.145 sec {6.9 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000004.png\n",
      "[Timer] data=0.014 forward=0.109 viz=0.012 total=0.136 sec {7.4 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000005.png\n",
      "[Timer] data=0.015 forward=0.102 viz=0.013 total=0.129 sec {7.7 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000006.png\n",
      "[Timer] data=0.016 forward=0.096 viz=0.013 total=0.124 sec {8.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000007.png\n",
      "[Timer] data=0.016 forward=0.092 viz=0.012 total=0.121 sec {8.3 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000008.png\n",
      "[Timer] data=0.016 forward=0.089 viz=0.012 total=0.118 sec {8.5 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000009.png\n",
      "[Timer] data=0.017 forward=0.087 viz=0.013 total=0.116 sec {8.6 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000010.png\n",
      "[Timer] data=0.017 forward=0.083 viz=0.012 total=0.112 sec {8.9 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000011.png\n",
      "[Timer] data=0.017 forward=0.082 viz=0.012 total=0.112 sec {9.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000012.png\n",
      "[Timer] data=0.017 forward=0.083 viz=0.012 total=0.111 sec {9.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000013.png\n",
      "[Timer] data=0.017 forward=0.083 viz=0.011 total=0.111 sec {9.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000014.png\n",
      "[Timer] data=0.017 forward=0.083 viz=0.011 total=0.111 sec {9.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000015.png\n",
      "[Timer] data=0.017 forward=0.082 viz=0.012 total=0.111 sec {9.0 FPS} \n",
      "Writing image to dump_demo_sequence\\matches_000000_000016.png\n",
      "Finished demo_superglue.py\n"
     ]
    }
   ],
   "source": [
    "# Define your main function to encapsulate the script logic\n",
    "def run_superglue_demo(input_dir, output_dir, resize, superglue_mode='indoor'):\n",
    "    import cv2\n",
    "    import matplotlib.cm as cm\n",
    "    import torch\n",
    "    from pathlib import Path\n",
    "    from models.matching import Matching\n",
    "    from models.utils import (AverageTimer, VideoStreamer, \n",
    "                              make_matching_plot_fast, frame2tensor)\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print('Running inference on device \"{}\"'.format(device))\n",
    "\n",
    "    config = {\n",
    "        'superpoint': {\n",
    "            'nms_radius': 4,\n",
    "            'keypoint_threshold': 0.005,\n",
    "            'max_keypoints': -1\n",
    "        },\n",
    "        'superglue': {\n",
    "            'weights': superglue_mode,\n",
    "            'sinkhorn_iterations': 20,\n",
    "            'match_threshold': 0.2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matching = Matching(config).eval().to(device)\n",
    "    keys = ['keypoints', 'scores', 'descriptors']\n",
    "\n",
    "    vs = VideoStreamer(input_dir, resize, 1, ['*.png', '*.jpg', '*.jpeg'], 1000000)\n",
    "    frame, ret = vs.next_frame()\n",
    "    assert ret, 'Error when reading the first frame (try different --input?)'\n",
    "\n",
    "    frame_tensor = frame2tensor(frame, device)\n",
    "    last_data = matching.superpoint({'image': frame_tensor})\n",
    "    last_data = {k+'0': last_data[k] for k in keys}\n",
    "    last_data['image0'] = frame_tensor\n",
    "    last_frame = frame\n",
    "    last_image_id = 0\n",
    "\n",
    "    if output_dir is not None:\n",
    "        print('==> Will write outputs to {}'.format(output_dir))\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    timer = AverageTimer()\n",
    "\n",
    "    while True:\n",
    "        frame, ret = vs.next_frame()\n",
    "        if not ret:\n",
    "            print('Finished demo_superglue.py')\n",
    "            break\n",
    "        timer.update('data')\n",
    "        stem0, stem1 = last_image_id, vs.i - 1\n",
    "\n",
    "        frame_tensor = frame2tensor(frame, device)\n",
    "        pred = matching({**last_data, 'image1': frame_tensor})\n",
    "        kpts0 = last_data['keypoints0'][0].cpu().numpy()\n",
    "        kpts1 = pred['keypoints1'][0].cpu().numpy()\n",
    "        matches = pred['matches0'][0].cpu().numpy()\n",
    "        confidence = pred['matching_scores0'][0].cpu().numpy()\n",
    "        timer.update('forward')\n",
    "\n",
    "        valid = matches > -1\n",
    "        mkpts0 = kpts0[valid]\n",
    "        mkpts1 = kpts1[matches[valid]]\n",
    "        color = cm.jet(confidence[valid])\n",
    "        text = [\n",
    "            'SuperGlue',\n",
    "            'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n",
    "            'Matches: {}'.format(len(mkpts0))\n",
    "        ]\n",
    "        k_thresh = matching.superpoint.config['keypoint_threshold']\n",
    "        m_thresh = matching.superglue.config['match_threshold']\n",
    "        small_text = [\n",
    "            'Keypoint Threshold: {:.4f}'.format(k_thresh),\n",
    "            'Match Threshold: {:.2f}'.format(m_thresh),\n",
    "            'Image Pair: {:06}:{:06}'.format(stem0, stem1),\n",
    "        ]\n",
    "        out = make_matching_plot_fast(\n",
    "            last_frame, frame, kpts0, kpts1, mkpts0, mkpts1, color, text,\n",
    "            path=None, show_keypoints=True, small_text=small_text)\n",
    "\n",
    "        timer.update('viz')\n",
    "        timer.print()\n",
    "\n",
    "        if output_dir is not None:\n",
    "            stem = 'matches_{:06}_{:06}'.format(stem0, stem1)\n",
    "            out_file = str(Path(output_dir, stem + '.png'))\n",
    "            print('\\nWriting image to {}'.format(out_file))\n",
    "            cv2.imwrite(out_file, out)\n",
    "\n",
    "    vs.cleanup()\n",
    "\n",
    "# Define your parameters\n",
    "input_dir = 'assets/freiburg_sequence/'\n",
    "output_dir = 'dump_demo_sequence'\n",
    "resize = [320, 240]\n",
    "\n",
    "# Call the function with the parameters\n",
    "run_superglue_demo(input_dir, output_dir, resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0c1f672-bb38-49a8-9e59-7f193a6c079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"indoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jack2\\Documents\\Final Project\\SuperGluePretrainedNetwork\\models\\superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n",
      "C:\\Users\\jack2\\Documents\\Final Project\\SuperGluePretrainedNetwork\\models\\superglue.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image0 Keypoints:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>577.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>353.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>248.0</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>334.0</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>410.0</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>544.0</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>781.0</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x      y\n",
       "0     134.0   14.0\n",
       "1     158.0   14.0\n",
       "2     577.0   14.0\n",
       "3     337.0   15.0\n",
       "4     353.0   15.0\n",
       "...     ...    ...\n",
       "1822  248.0  759.0\n",
       "1823  334.0  759.0\n",
       "1824  410.0  759.0\n",
       "1825  544.0  759.0\n",
       "1826  781.0  759.0\n",
       "\n",
       "[1827 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image1 Keypoints:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1104.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>658.0</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>922.0</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>1054.0</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1115.0</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>1148.0</td>\n",
       "      <td>959.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1045 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x      y\n",
       "0      191.0    8.0\n",
       "1      211.0    8.0\n",
       "2      365.0    8.0\n",
       "3     1104.0    8.0\n",
       "4      295.0    9.0\n",
       "...      ...    ...\n",
       "1040   658.0  959.0\n",
       "1041   922.0  959.0\n",
       "1042  1054.0  959.0\n",
       "1043  1115.0  959.0\n",
       "1044  1148.0  959.0\n",
       "\n",
       "[1045 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from models.superpoint import SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize models\n",
    "superpoint = SuperPoint(config={})\n",
    "superglue = SuperGlue(config={})\n",
    "\n",
    "def extract_feature_point(image):\n",
    "    \"\"\"Extract feature points and descriptors from the image\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Grayscale(),  # Convert image to grayscale\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = superpoint({\"image\": image_tensor})\n",
    "    \n",
    "    keypoints = pred[\"keypoints\"][0].clone().detach()\n",
    "    descriptors = pred[\"descriptors\"][0].clone().detach()\n",
    "    scores = pred[\"scores\"][0].clone().detach() if \"scores\" in pred else None\n",
    "\n",
    "    return keypoints, descriptors, scores\n",
    "\n",
    "def match_features(image0, image1):\n",
    "    \"\"\"Match feature points between two images\"\"\"\n",
    "    K0, d0, s0 = extract_feature_point(image0)\n",
    "    K1, d1, s1 = extract_feature_point(image1)\n",
    "\n",
    "    if s0 is None or s1 is None:\n",
    "        raise ValueError(\"SuperPoint did not return scores for keypoints.\")\n",
    "\n",
    "    data = {\n",
    "        'keypoints0': K0.unsqueeze(0),\n",
    "        'keypoints1': K1.unsqueeze(0),\n",
    "        'descriptors0': d0.unsqueeze(0),\n",
    "        'descriptors1': d1.unsqueeze(0),\n",
    "        'scores0': s0.unsqueeze(0),\n",
    "        'scores1': s1.unsqueeze(0),\n",
    "        'image0': T.Grayscale()(T.ToTensor()(image0)).unsqueeze(0),\n",
    "        'image1': T.Grayscale()(T.ToTensor()(image1)).unsqueeze(0)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        matches = superglue(data)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from the path\"\"\"\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "def display_keypoints(keypoints, title=\"keypoints\"):\n",
    "    df = pd.DataFrame(keypoints, columns = ['x', 'y'])\n",
    "    display(df)\n",
    "def keypoints_to_tensor(keypoints):\n",
    "    return torch.tensor(keypoints)\n",
    "\n",
    "def visualize_matches(image0, image1, keypoints0, keypoints1, matches):\n",
    "    \"\"\"Visualize the feature points and matches on the images\"\"\"\n",
    "    # Convert images to numpy arrays for plotting\n",
    "    image0_np = T.ToTensor()(image0).permute(1, 2, 0).numpy()\n",
    "    image1_np = T.ToTensor()(image1).permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    # Plot image0 with keypoints\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image0_np)\n",
    "    plt.title('Image0 with Keypoints')\n",
    "    plt.scatter(keypoints0[:, 0], keypoints0[:, 1], c='r', s=10, label='Keypoints')\n",
    "    # for i, (x, y) in enumerate(keypoints0):\n",
    "    #     plt.text(x,y, f'{i}', color = 'blue', fontsize = 8, ha = 'right')\n",
    "\n",
    "    # Plot image1 with keypoints\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image1_np)\n",
    "    plt.title('Image1 with Keypoints')\n",
    "    plt.scatter(keypoints1[:, 0], keypoints1[:, 1], c='r', s=10, label='Keypoints')\n",
    "    # for i, (x, y) in enumerate(keypoints1):\n",
    "    #     plt.text(x,y, f'{i}', color = 'blue', fontsize = 8, ha = 'right')\n",
    "\n",
    "    plt.savefig('feature_point.png')  # Save the image with keypoints\n",
    "    plt.close()  # Close the plot\n",
    "\n",
    "# Load images\n",
    "image0 = load_image('assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg')\n",
    "image1 = load_image('assets/scannet_sample_images/scene0711_00_frame-001995.jpg')\n",
    "\n",
    "# Extract feature points and match features\n",
    "K0, d0, s0 = extract_feature_point(image0)\n",
    "K1, d1, s1 = extract_feature_point(image1)\n",
    "matches = match_features(image0, image1)\n",
    "\n",
    "# Visualize matches\n",
    "visualize_matches(image0, image1, K0.cpu().numpy(), K1.cpu().numpy(), matches)\n",
    "print(\"Image0 Keypoints:\")\n",
    "display_keypoints(K0)\n",
    "\n",
    "print(\"Image1 Keypoints:\")\n",
    "display_keypoints(K1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f47ecd6f-cd4c-4d05-b7e5-5f8ff54caef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[134.,  14.],\n",
       "         [158.,  14.],\n",
       "         [577.,  14.],\n",
       "         [337.,  15.],\n",
       "         [353.,  15.],\n",
       "         [754.,  16.],\n",
       "         [400.,  22.],\n",
       "         [967.,  22.],\n",
       "         [513.,  23.],\n",
       "         [575.,  24.]]),\n",
       " tensor([[ 191.,    8.],\n",
       "         [ 211.,    8.],\n",
       "         [ 365.,    8.],\n",
       "         [1104.,    8.],\n",
       "         [ 295.,    9.],\n",
       "         [ 301.,    9.],\n",
       "         [ 309.,    9.],\n",
       "         [ 296.,   24.],\n",
       "         [ 223.,   25.],\n",
       "         [ 332.,   26.]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K0.squeeze()[:10], K1.squeeze()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a49d7861-9bb6-4d36-b7f3-98b023e4fe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1100, 2]), torch.Size([1045, 2]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K0.shape, K1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e83320b6-2e6d-4d98-86d0-45d97cff915e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0483, 0.0071, 0.0091, 0.0656, 0.0089, 0.0194, 0.0398, 0.0387, 0.0067,\n",
       "        0.0231])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151ac28-4b11-4b3e-847d-a7f56c92a767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
